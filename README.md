# Foundations of Deep Learning  
**Neural Networks, Optimization, and Regularization**

## ğŸ“˜ Project Overview
This project is a structured, hands-on learning notebook that introduces the **foundations of deep learning** using **TensorFlow / Keras**.  
It combines theory, coding, visualization, and experimental tasks to explain how neural networks learn, optimize, and generalize.

The experiments are based on the **MNIST handwritten digit dataset** and progress from a simple neural network to advanced concepts such as regularization, early stopping, and optimizer comparison.

---

## ğŸ“‚ Repository Structure
project/  
â”œâ”€â”€ notebook.ipynb  
â”œâ”€â”€ README.md  
â”œâ”€â”€ results/  
â”‚   â”œâ”€â”€ predictions/  
â”‚   â”œâ”€â”€ loss_curves/  
â”‚   â””â”€â”€ optimizer_tests/  
â””â”€â”€ submission/  
    â”œâ”€â”€ Task01_PredictionAnalysis.md  
    â”œâ”€â”€ Task02_CustomDigit.md  
    â”œâ”€â”€ Task03_Epochs.md  
    â”œâ”€â”€ Task04_EarlyStopping.md  
    â”œâ”€â”€ Task05_Dropout.md  
    â”œâ”€â”€ Task06_L2.md  
    â”œâ”€â”€ Task07_Optimizers.md  
    â”œâ”€â”€ Task08_BatchSize.md  
    â”œâ”€â”€ Task09_Activations.md  
    â””â”€â”€ Task10_Weights.md  

---

## ğŸ§  Notebook Sections

### Section 1 â€” Building & Training a Neural Network
- Load and normalize MNIST data  
- Build a fully connected neural network  
- ReLU and Softmax activations  
- Train using the Adam optimizer  
- Visualize predictions and training curves  

**Goal:**  
Understand forward pass, loss computation, and basic model behavior.

---

### Section 2 â€” Regularization & Training Control
- Proper dataset splitting (train / validation / test)  
- Dropout regularization  
- L2 (weight decay) regularization  
- EarlyStopping callback  
- Final evaluation on unseen test data  

**Goal:**  
Learn how to reduce overfitting and improve model generalization.

---

### Section 3 â€” Student Tasks & Experiments
- Prediction behavior analysis  
- Custom handwritten digit generalization test  
- Epoch comparison (5, 10, 20 epochs)  
- Dropout ablation study  
- L2 regularization tuning  
- Optimizer comparison (SGD, Momentum, Adam, AdamW)  
- Batch size experiments  
- Activation function comparison  
- Weight inspection and model capacity analysis  

**Goal:**  
Build intuition about optimization dynamics, regularization strength, and architectural choices.

---

## âš™ï¸ Requirements
pip install tensorflow numpy matplotlib opencv-python

---

## â–¶ï¸ How to Run
1. Clone the repository  
git clone <your-repository-url>  

2. Navigate to the project directory  
cd project  

3. Open the notebook  
jupyter notebook notebook.ipynb  

---

## ğŸ“Š Results
- Training & validation loss/accuracy plots  
- Prediction visualizations  
- Optimizer comparison curves  
- Regularization impact analysis  

All outputs should be saved inside the `results/` directory.

---

## ğŸ¯ Learning Outcomes
By completing this project, you will:
- Understand how neural networks learn representations  
- Analyze training and validation curves  
- Compare optimizers and regularization methods  
- Identify overfitting and underfitting  
- Gain practical intuition for deep learning workflows  

---

## âœ… Submission Notes
- The notebook must run without errors  
- Follow the required folder structure  
- Each task must be documented in Markdown  
- Keep explanations concise and clear  

---

## ğŸ“Œ Final Note
This project focuses on **understanding model behavior**, not just achieving high accuracy.  
Clear explanations and well-organized experiments are key.
